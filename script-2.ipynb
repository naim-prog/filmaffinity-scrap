{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf02fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import cloudscraper\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f218335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c8faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_headless_driver():\n",
    "    \"\"\"Setup Chrome driver in headless mode for scraping\"\"\"\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Headless mode - no browser popup\n",
    "    chrome_options.add_argument('--headless')\n",
    "    \n",
    "    # User agent and language settings\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36')\n",
    "    chrome_options.add_argument('--accept-language=es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3')\n",
    "    \n",
    "    # Anti-detection settings\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Performance settings\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    \n",
    "    # Proxy settings (uncomment if needed)\n",
    "    # chrome_options.add_argument('--proxy-server=http://42.118.0.24:16000')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    # Hide webdriver property\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15379637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(driver, url, wait_time=3):\n",
    "    \"\"\"\n",
    "    Scrape a single page and return BeautifulSoup object\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        url: URL to scrape\n",
    "        wait_time: Time to wait for page load (seconds)\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Navigate to the page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Wait for body element to be present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        # Get page source\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Create BeautifulSoup object\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "        \n",
    "    except TimeoutException:\n",
    "        print(f\"‚úó Timeout while loading: {url}\")\n",
    "        return None\n",
    "    except WebDriverException as e:\n",
    "        print(f\"‚úó WebDriver error for {url}: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Unexpected error for {url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6443eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html_to_file(content, section, page_num, output_dir=\"film_reviews\"):\n",
    "    \"\"\"\n",
    "    Save HTML content to a file, handling different content types and encodings\n",
    "    \n",
    "    Args:\n",
    "        content: HTML content (string) or response object\n",
    "        section: Section identifier\n",
    "        page_num: Page number\n",
    "        output_dir: Directory to save files\n",
    "    \n",
    "    Returns:\n",
    "        File path where HTML was saved\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"{section}_{page_num}.html\"\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        # If content is a response object, get the text with proper encoding\n",
    "        if hasattr(content, 'text'):\n",
    "            html_content = content.text\n",
    "        elif hasattr(content, 'content'):\n",
    "            # Try to decode content with the response's encoding or utf-8\n",
    "            encoding = content.encoding if hasattr(content, 'encoding') else 'utf-8'\n",
    "            html_content = content.content.decode(encoding)\n",
    "        else:\n",
    "            # If it's already a string\n",
    "            html_content = str(content)\n",
    "        \n",
    "        # Save HTML content\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"üíæ Saved HTML to: {file_path}\")\n",
    "        return file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_cloudscraper(url):\n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\n",
    "            'browser': 'chrome',\n",
    "            'platform': 'windows',\n",
    "            'mobile': False\n",
    "        },\n",
    "        delay=10  # 10 seconds delay between requests\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.3',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "    }\n",
    "    \n",
    "    # Add headers to the session\n",
    "    scraper.headers.update(headers)\n",
    "    \n",
    "    # Make the request with timeout\n",
    "    response = scraper.get(url, 15)  # 10 seconds timeout for the request\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd1bd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_filmaffinity_sections_and_pages(film_links, base_url_template, save_html=True):\n",
    "    \"\"\"\n",
    "    Scrape multiple sections and pages from FilmAffinity\n",
    "    \n",
    "    Args:\n",
    "        sections: List of section identifiers\n",
    "        pages_per_section: Number of pages to scrape per section\n",
    "        base_url_template: URL template with placeholders for section and page\n",
    "        save_html: Whether to save HTML files to disk\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with scraped data organized by section and page\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # driver = setup_headless_driver()\n",
    "    \n",
    "    list_end_reviews = open('film_reviews/end_reviews', 'r').read().split('\\n')\n",
    "\n",
    "    try:\n",
    "        # Outer loop: iterate through sections\n",
    "        for link in (pbar:=tqdm(list(film_links))):\n",
    "\n",
    "            film_id = str(link).split('/')[-1].split('.')[0]\n",
    "            review_page_id = 1\n",
    "\n",
    "            pbar.set_postfix_str(film_id)\n",
    "\n",
    "            if film_id in list_end_reviews:\n",
    "                continue\n",
    "\n",
    "            # Inner loop: iterate through pages for each section\n",
    "            while True:\n",
    "                \n",
    "                if not os.path.exists(os.path.join(os.getcwd(), \"film_reviews\", film_id + \"_\" + str(review_page_id) + \".html\")):\n",
    "                    \n",
    "                    # Build URL for current section and page\n",
    "                    url = base_url_template.format(section=film_id.removeprefix(\"film\"), page=review_page_id)\n",
    "\n",
    "                    # Scrape the page\n",
    "                    # soup = scrape_page(driver, url)\n",
    "                    response = scrape_with_cloudscraper(url)\n",
    "\n",
    "                    soup = BeautifulSoup(response.text)\n",
    "\n",
    "                    # Error 404, no more reviews for this film\n",
    "                    if response.status_code == 404:\n",
    "                        with open(\"film_reviews/end_reviews\", 'a') as file_end:\n",
    "                            file_end.write(f'{film_id}\\n')\n",
    "\n",
    "                        break\n",
    "\n",
    "\n",
    "                    # Optionally save HTML to file\n",
    "                    if save_html:\n",
    "                        save_html_to_file(response.text, film_id, review_page_id)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    with open(f'film_reviews/{film_id}_{review_page_id}.html', 'r', encoding=\"utf-8\") as file:\n",
    "                        response = file.read()\n",
    "                    soup = BeautifulSoup(response)\n",
    "\n",
    "\n",
    "                h1_s = soup.find_all(\"h1\")\n",
    "\n",
    "                too_many_requests = False\n",
    "                for h1 in h1_s:\n",
    "                    # If too many request get a proxy and continue requesting films\n",
    "                    if h1.get_text() == \"Too many requests\":\n",
    "                        too_many_requests = True\n",
    "                        print(f\"INFO: Too many requests\\n\")\n",
    "                \n",
    "\n",
    "                # Delete the 4KB response\n",
    "                if too_many_requests:\n",
    "                    os.remove(f'film_reviews/{film_id}_{review_page_id}.html')\n",
    "                    sys.exit()\n",
    "                    continue\n",
    "\n",
    "                review_page_id += 1                    \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scraping: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Always close the driver\n",
    "        print(\"üîö Closing browser...\")\n",
    "        # driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f70b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of films: 24367\n"
     ]
    }
   ],
   "source": [
    "# Example usage - customize these parameters for your needs\n",
    "\n",
    "# Define the sections you want to scrape (user IDs or section identifiers)\n",
    "film_links = [\"https://www.filmaffinity.com/es/\" + file_name for file_name in os.listdir('htmls_film_info')]\n",
    "print(f\"Number of films: {len(film_links)}\")\n",
    "\n",
    "# URL template - modify this based on the actual FilmAffinity URL structure\n",
    "# Use {section} and {page} as placeholders\n",
    "# base_url_template = \"https://www.filmaffinity.com/es/userratings.php?user_id={section}&p={page}&orderby=rating-date&chv=grid\"\n",
    "base_url_template = \"https://www.filmaffinity.com/es/reviews/{page}/{section}.html\"\n",
    "\n",
    "# Alternative URL templates you might need:\n",
    "# base_url_template = \"https://www.filmaffinity.com/es/films.php?section={section}&p={page}\"\n",
    "# base_url_template = \"https://www.filmaffinity.com/es/search.php?category={section}&p={page}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21424276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting the scraping process...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12770/24367 [00:26<00:24, 479.35it/s, film576352] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during scraping: ('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupci√≥n de una conexi√≥n existente por el host remoto', None, 10054, None))\n",
      "üîö Closing browser...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the scraping process\n",
    "print(\"üöÄ Starting the scraping process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scraped_data = scrape_filmaffinity_sections_and_pages(\n",
    "    film_links=film_links,\n",
    "    base_url_template=base_url_template,\n",
    "    save_html=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
